{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/onnx.png\" width=700 align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX (Open Neural Network eXchange)\n",
    "\n",
    "Originally created by Facebook and Microsoft as an industry collaboration for import/export of neural networks\n",
    "* ONNX has grown to include support for \"traditional\" ML models\n",
    "* interop with many software libraries\n",
    "* has both software (CPU, optional GPU accelerated) and hardware (Intel, Qualcomm, etc.) runtimes.\n",
    "\n",
    "https://onnx.ai/\n",
    "\n",
    "* DAG-based model\n",
    "* Built-in operators, data types\n",
    "* Extensible -- e.g., ONNX-ML\n",
    "* Goal is to allow tools to share a single model format\n",
    "\n",
    "*Of the \"standard/open\" formats, ONNX clearly has the most momentum in the past year or two.*\n",
    "\n",
    "## Viewing a Model\n",
    "\n",
    "ONNX models are not directly (as raw data) human-readable, but, as they represent a graph, can easily be converted into textual or graphical representations.\n",
    "\n",
    "Here is a snippet of the [SqueezeNet](https://arxiv.org/abs/1602.07360) image-recognition model, as rendered in the ONNX visualization tutorial at https://github.com/onnx/tutorials/blob/master/tutorials/VisualizingAModel.md. \n",
    "\n",
    "> The ONNX codebase comes with the visualization converter used in this example -- it's a simple script currently located at https://github.com/onnx/onnx/blob/master/onnx/tools/net_drawer.py\n",
    "\n",
    "<img src='images/squeezenet.png'>\n",
    "\n",
    "### Let's Build a Model and Convert it to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "data = pd.read_csv('data/diamonds.csv')\n",
    "X = data.carat\n",
    "y = data.price\n",
    "model = LinearRegression().fit(X.values.reshape(-1,1), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ONNX can be generated from many modeling tools. A partial list is here: https://github.com/onnx/tutorials#converting-to-onnx-format\n",
    "\n",
    "Microsoft has contributed a lot of resources toward open-source ONNX capabilities, including, in early 2019, support for Apache Spark ML Pipelines: https://github.com/onnx/onnxmltools/blob/master/onnxmltools/convert/sparkml/README.md\n",
    "\n",
    "__Convert to ONNX__\n",
    "\n",
    "Note that we can print a string representation of the converted graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "\n",
    "initial_type = [('float_input', FloatTensorType([1, 1]))]\n",
    "onx = convert_sklearn(model, initial_types=initial_type)\n",
    "print(onx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save it as a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"diamonds.onnx\", \"wb\") as f:\n",
    "    f.write(onx.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file itself is binary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head diamonds.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Do We Consume ONNX and Make Predictions?\n",
    "\n",
    "One of the things that makes ONNX a compelling solution in 2019 is the wide industry support not just for model creation, but also for performant model inference.\n",
    "\n",
    "Here is a partial list of tools that consume ONNX: https://github.com/onnx/tutorials#scoring-onnx-models\n",
    "\n",
    "Of particular interest for productionizing models are\n",
    "* Apple CoreML\n",
    "* Microsoft `onnxruntime` and `onnxruntime-gpu` for CPU & GPU-accelerated inference\n",
    "* TensorRT for NVIDIA GPU\n",
    "* Conversion for Qualcomm Snapdragon hardware: https://developer.qualcomm.com/docs/snpe/model_conv_onnx.html\n",
    "\n",
    "Today, we'll look at \"regular\" server-based inference with a sample REST server, using `onnxruntime`\n",
    "\n",
    "#### We'll start by loading the `onnxruntime` library, and seeing how we make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as rt\n",
    "\n",
    "sess = rt.InferenceSession(\"diamonds.onnx\")\n",
    "\n",
    "print(\"In\", [(i.name, i.type, i.shape) for i in sess.get_inputs()])\n",
    "  \n",
    "print(\"Out\", [(i.name, i.type, i.shape) for i in sess.get_outputs()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've skipped some metadata annotation in the model creation for this quick example -- that's why our input field name is \"float_input\" and the output is called \"variable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sample_to_score = np.array([[1.0]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = sess.run(['variable'], {'float_input': sample_to_score})\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At this point, we can build our service...\n",
    "\n",
    "Now we are free to choose our service infrastructure of choice.\n",
    "\n",
    "Moreover, we can containerize that service, so we get back all of the benefits of Docker, Kubernetes, etc.\n",
    "\n",
    "But this time, we have a minimal serving infrastructure that knows only about the model itself, and loads models in a single, open, industry-standard format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros and Cons: ONNX\n",
    "* Most major deep learning tools have ONNX support\n",
    "* MIT license makes it both OSS and business friendly\n",
    "* Seems to achieve its first-order goal of allowing tools interop for neural nets\n",
    "* As of 2019, is the closest thing we have to an open, versatile, next-gen format *with wide support*\n",
    "* Protobuf format is compact and typesafe\n",
    "* Biggest weakness was \"classical\" ML and feature engineering support -- this has now been fixed\n",
    "* Microsoft open-sourced (Dec 2018) a high-perf runtime (GPU, CPU, language bindings, etc.) https://azure.microsoft.com/en-us/blog/onnx-runtime-is-now-open-source/\n",
    "  * Being used as part of Windows ML / Azure ML\n",
    "  * https://github.com/Microsoft/onnxruntime\n",
    "* In Q1-Q2 of 2019, Microsoft added a Spark ML Pipeline exporter to the `onnxmltools` project\n",
    "  * https://github.com/onnx/onnxmltools\n",
    "\n",
    "Cons:\n",
    "* Focused on DAG-friendly neural networks (i.e., not intended to be fully general)\n",
    "* Wasn't originally intended as a deployment format *per se*\n",
    "  * Doesn't have a standard or reference runtime\n",
    "  * Doesn't provide certification or standard around correctness\n",
    "  * No opinion on security, etc.\n",
    "* Protobuf format is not human readable or manageable via text-oriented tooling\n",
    "  * Though the graph itself can be (e.g., PyTorch export output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "name": "02-ONNX-Models",
  "notebookId": 2375086480049026
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
