{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Machine Learning Projects in Production with Open Standard Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the Scene: Core Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of machine learning...\n",
    "\n",
    "* __Training__ is the process of obtaining a (hopefully useful) machine learning model from data\n",
    "* __Inference__ or __Prediction__ or __Scoring__ is the process of using a model to obtain a __score__ for some new data encountered in your business or research\n",
    "* Sometimes (e.g., in __online learning__ use cases) the same system is performing training and inference at approximately the same time\n",
    "\n",
    "In this session, we will __not__ focus on training (or data preparation, cleaning, model tuning, selection, etc.) \n",
    "\n",
    "There are lots of great resources focusing on that phase of work, and we're going to assume that you already have a model you're happy with (or at least a process for creating those models).\n",
    "\n",
    "So you've trained and tuned a model, done some validation tests to ensure it generates appropriate predictions on new data, and you want to deploy this model into an enterprise-scale inference service, where it can deliver predictions for the business. \n",
    "\n",
    "> For example, you've trained a great recommender system, and *now it needs to be exposed as a scalable service* consumed by your company's online-store app, which will send carts of products to that recommender, and receive back recommended products to offer  customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Source Pre-History: Proprietary Inference Servers\n",
    "\n",
    "Why is this a challenge today?\n",
    "\n",
    "For a long time, businesses using machine learning employed proprietary tools like SAS, SPSS, and FICO to perform modeling.\n",
    "\n",
    "Many of these products and vendors licensed proprietary \"model servers\" or \"inference servers\" which were created specifically to take models and expose them elsewhere in the IT infrastructure as a service.\n",
    "\n",
    "If your company was a customer of these products, the enterprise \"solution\" included both the data mining tools (modeling) and the serving tools (inference)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Rise of Open Source: Stone Age\n",
    "\n",
    "As open-source data science tools rose in prominence over the last decade, more data scientists, statisticians, researchers, and analysts began relying on\n",
    "* Python\n",
    "    * SciPy stack\n",
    "    * scitkit-learn\n",
    "    * TensorFlow\n",
    "    * etc.\n",
    "* R\n",
    "    * dplyr\n",
    "    * ggplot2\n",
    "    * etc.\n",
    "* Spark, H2O, others...\n",
    "\n",
    "As we've all seen, the cycle of research, development, publication, and open-source tooling has led to a huge explosion of data-driven uses throughout the world.\n",
    "\n",
    "__But__ none of those tools had a clear, complete story for how to deploy a model once it was trained.\n",
    "\n",
    "So engineers carved out the *Stone-Age Solution* ... namely, attempt to wrap the data science stack in a lightweight web service framework, and put it into production.\n",
    "\n",
    "The classic example is a Python [Flask](https://en.wikipedia.org/wiki/Flask_(web_framework)) web endpoint that wraps a call to scikit-learn's `model.predict(...)`\n",
    "\n",
    "Before discussing the many drawbacks of this approach, let's quickly review..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Source: Bronze Age\n",
    "\n",
    "Since model inference is typically lightweight, stateless, and idempotent, it is an ideal candidate for a scale-out containerized service using a container scaling framework like Kubernetes.\n",
    "\n",
    "The \"Bronze Age\" of open-source model deployment containerized the Stone Age approach, making it easy to scale, robust, etc.\n",
    "\n",
    "Containerization was definitely an improvement ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Source: Platform Gold Rush\n",
    "\n",
    "<img src='images/gold.jpg'>\n",
    "\n",
    "Businesses realized that they wanted enterprise manageability over these ML inference services ... \n",
    "\n",
    "And a lot of entrepreneurs realized that making money in novel ML training was hard (after all, thousands of Ph.D. researchers were working on the same problems, and giving the results away for free) ... but making a \"platform\" that\n",
    "* Dockerized open-source ML stacks\n",
    "* Deployed them on-prem or in the cloud via Kubernetes\n",
    "* and provided some manageability (\"ML Ops\") \n",
    "was both easy and lucrative.\n",
    "\n",
    "### 2018-2019 will go down as the ML Ops Gold Rush\n",
    "\n",
    "And, as in the California Gold Rush, it has been easier selling tools and services than finding actual gold.\n",
    "\n",
    "__ML deployment platforms *do* have value to offer__ and we'll come back to that part. But first we need to focus on the Achilles heel, namely Dockerized data science stacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You Wouldn't Deploy an Enterprise Service by Putting Your Dev Machine in the Datacenter\n",
    "\n",
    "<img src='images/dock.jpg'>\n",
    "\n",
    "## So Why Would You Deploy a ML Service by Putting the ML Stack in a Container?\n",
    "\n",
    "\"It works (for now)\" is about the best thing you can say about such a deployment.\n",
    "\n",
    "Meanwhile, how do we address...\n",
    "\n",
    "* model inspection\n",
    "* versioning\n",
    "* diffing model versions\n",
    "* porting to other environments (e.g., ARM vs. Intel or mobile vs. client vs. server)\n",
    "* using the model in an alternate runtime (e.g., a scikit-learn model in a Spark job)\n",
    "* using models *from* an alternate runtime (e.g., Spark cannot natively export a ML pipeline to a containerizable service)\n",
    "* updating dependencies (e.g., patching a security vulnerability in underlying components https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-6446)\n",
    "* not to mention lots of design issues like...\n",
    "    * Why should a ML model (which is typically a limited set of math operations) be deployed as a full computing stack and environment?\n",
    "    * Why should we use an enormous container and billions of compute cycles to perform arithmetic and a bit of trigonometry?\n",
    "    \n",
    "### Fundamentally, Containerizing a ML Stack (with Model) Violates Separation of Concerns\n",
    "\n",
    "Consider: we send each other plain-text emails, which can be written and read according to standard text encodings, rather than, say, the absurd idea of sending executable VM Images with an OS and a word processor, along with document in the word processor's proprietary format\n",
    "\n",
    "The ML model, once trained, can be viewed as data. \n",
    "\n",
    "It should be possible to \n",
    "* manage this data using standard, well known data-management tools and practices\n",
    "* create this data using any compliant tool\n",
    "* consume this data using any compliant tool\n",
    "* validate that this data has a single universal interpretation\n",
    "    * why is this important? consider the impact of tiny difference in implementation of, say, *ln(x)* on inference at scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Can We Address this Separation-of-Concerns Problem?\n",
    "\n",
    "__Get the model *out* of the model-creation environment (both logically and physically)__\n",
    "\n",
    "Physically: create a separate entity like a file\n",
    "\n",
    "Logically: ensure that entity is independent -- so saving a scikit-learn model as a pickle file (which will later need scikit-learn after being unpickled) does not count as a solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Kind of Separate File Do We Want?\n",
    "\n",
    "Ideally, we'd like a format that is ...\n",
    "* an open, cross-industry standard\n",
    "* not owned or controlled by any one organization\n",
    "* not encumbered by intellectual property restrictions (licensing rules)\n",
    "* works with many ML tools, on many platforms\n",
    "* time/space efficient\n",
    "* robust (can support many kinds of ML models, including future types)\n",
    "* consistent (produces the same output for the same model, no matter the deployment OS, architecture, etc.)\n",
    "* simple (does not support unnecessary operations)\n",
    "* secure (minimizes attack surface by design, offers verifiability, etc)\n",
    "* is human readable (or can be made human readable)\n",
    "* can be managed in any database, content-management system, source control, etc.\n",
    "\n",
    "*As in most engineering scenarios, there is no single, magical solution that hits every bullet-point*\n",
    "\n",
    "But there are number of approaches which offer many of these attributes and which are worthy of consideration.\n",
    "\n",
    "This session looks at several of these tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
